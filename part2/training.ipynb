{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dac026",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: Training and Testing\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0353e75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**DIVE into Deep Learning**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0333f64-e153-4220-9abf-9c52c67f6ec2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b14dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Optimization Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06d89a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why can we learn from examples?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41c47a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::{important}\n",
    "For the problem to be meaningful/non-trivial, $(\\RM{x},\\R{y})$ is assumed to be *random* with some *unknown joint distribution* $p_{\\RM{x},\\R{y}}$.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433122b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why?\n",
    "- If $\\R{y}$ were deterministic, i.e., $\\R{y}=y$ all the time, then the classifier can simply return $y$ without even looking at $\\RM{x}$.\n",
    "- If $p_{\\RM{x},\\R{y}}$ were known instead, then $p_{\\R{y}|\\RM{x}}$ would also be known and therefore needed not be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df646266",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "::::{important}\n",
    "Examples (or samples) are often collected *randomly* and *independently*, and they are identically distributed as $p_{\\R{x},y}$. \n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb39ad",
   "metadata": {},
   "source": [
    "More precisely, the examples are called [i.i.d. samples](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) of $(\\RM{x},\\R{y})$, written as\n",
    "\n",
    "$$(\\RM{x}_1,\\R{y}_1), (\\RM{x}_2,\\R{y}_2), \\dots\\sim_{\\text{i.i.d.}} p_{\\RM{x},\\R{y}},$$\n",
    "\n",
    "which means that their joint distribution is $p_{\\RM{x}_1,\\R{y}_1, \\RM{x}_2,\\R{y}_2, \\dots}(x_1,y_1,x_2,y_2,\\dots) = p_{\\RM{x},\\R{y}}(x_1,y_1) p_{\\RM{x},\\R{y}}(x_2,y_2)\\cdots$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab73cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why?\n",
    "- If all the examples were the same instead, they could not show the pattern of how $\\R{y}$ depends on $\\RM{x}$.\n",
    "- Noise in individual examples can be smoothed out by averaging out the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76851c49",
   "metadata": {},
   "source": [
    "::::{seealso}\n",
    "We can learn an unknown distribution from its i.i.d. samples by the [(Uniform) Law of Large Number (LLN)](https://en.wikipedia.org/wiki/Law_of_large_numbers).\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa9cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How to determine if a classifier is good?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff13844",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ultimately, we desire a classifier with the maximum accuracy in predicting $\\R{y}$ but doing so is [computationally too difficult](https://en.wikipedia.org/wiki/Loss_functions_for_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b1de4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, we regard a classification algorithm to be reasonably good if \n",
    "- it can achieve the maximum possible accuracy \n",
    "- as the number of training samples goes to $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fd57a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ":::{prf:definition}\n",
    ":label: classifier\n",
    "\n",
    "A probabilistic classifier for the input feature $\\RM{x}$ and label $\\R{y}$ with unknown joint distribution  is a conditional pmf\n",
    "\n",
    "$$\n",
    "\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})\\qquad \\text{for }\\M{x}\\in \\mc{X}, y\\in \\mc{Y}\n",
    "$$\n",
    "\n",
    "defined as a function of the i.i.d. samples\n",
    "\n",
    "$$\\{(\\RM{x}_i,\\R{y}_i)\\}_{i=1}^N$$\n",
    "\n",
    "of $(\\RM{x},\\R{y})$ (but independent of $(\\RM{x},\\R{y})$). \n",
    "The classifier is said to be a *consistent* estimate (of $p_{\\R{y}|\\M{x}}$) if\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{N\\to \\infty} \\Pr\\Set{\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x})\\text{ for all } y\\in \\mc{Y}}=1,\\tag{consistency}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "namely, $\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})$ converges almost surely (a.s.) to $p_{\\R{y}|\\RM{x}}$. $\\square$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5950f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A consistent probabilistic classifier gives rise to an asymptotically optimal hard-decision classifier that achieves the maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f7ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ":::{prf:proposition}\n",
    ":label: performance\n",
    "\n",
    "If for some $\\epsilon\\geq 0$ that\n",
    "\n",
    "$$\\Pr\\Set{\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x}) \\text{ for all } y\\in \\mc{Y}}=1-\\epsilon,$$\n",
    "\n",
    "the (hard-decision) classifier\n",
    "\n",
    "$$\\begin{align}\\R{f}(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} \\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})\\tag{hard decision}\\end{align}$$\n",
    "\n",
    "achieves an accuracy\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sup_{f:\\mc{X}\\to \\mc{Y}} \\Pr(\\R{y}= f(\\RM{x})) &\\geq E\\left[\\max_{y\\in \\mc{Y}} p_{\\R{y}|\\M{x}}(y|\\RM{x})\\right] - \\epsilon,\\tag{accuracy lower bound}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the expectation is the maximum possible accuracy. $\\square$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea56de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{prf:proof}\n",
    "\n",
    "For any classifier $f$,\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\Pr(\\R{y}= f(\\RM{x}))\n",
    "&= E[p_{\\R{y}|\\M{x}}(f(\\RM{x})|\\RM{x})] \\\\\n",
    "&\\leq E\\left[\\max_{y\\in \\mc{Y}} p_{\\R{y}|\\M{x}}(y|\\RM{x})\\right]\\end{align*}\n",
    "$$\n",
    "\n",
    "where the last inequality is achievable with equality with the hard-decision classifier and $\\R{q}$ replaced by $p$. This implies the desired accuracy lower bound for the case $\\epsilon=0$. The more general case with $\\epsilon\\geq 0$ can be derived similarly. $\\blacksquare$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17c036",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How can we obtain a consistent classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1abf3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We train a neural network to minimize certain *loss* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d1011",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common loss function for classification uses the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) measure in information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e69742",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "::::{important}\n",
    "\n",
    "The theoretical underpinning is the following identity that relates three information quantities:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\overbrace{E\\left[\\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{ \\text{Cross entropy}\\\\ H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=} &\\equiv \\overbrace{E\\left[\\log \\frac{1}{p_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{\\text{Conditional entropy}\\\\ H(\\R{y}|\\RM{x}):=} + \\overbrace{E\\left[\\log \\frac{p_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right].}^{\\text{Divergence}\\\\ D(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=}\\tag{identity}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1a5ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The identity can be proved quite easily using the linearity of expectation \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "E[\\R{u}+\\R{v}]=E[\\R{u}]+E[\\R{v}] \\tag{linearity of expectation}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and a property of logarithm that $\\log uv = \\log u+ \\log v$ for all $u,v>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be03c1",
   "metadata": {},
   "source": [
    "::::{card}\n",
    ":header: [open in new tab](https://www.cs.cityu.edu.hk/~ccha23/dl/InformationIdentity.mp4)\n",
    ":::{iframe} https://www.cs.cityu.edu.hk/~ccha23/dl/InformationIdentity.mp4\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f1db0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{prf:proposition}\n",
    ":label: consistency\n",
    "\n",
    "With $q_{\\R{y}|\\RM{x}}(y|\\M{x})$ being a valid pmf of a random variable taking values from $\\mc{Y}$ conditioned on a random variable taking values from $\\mc{X}$, \n",
    "\n",
    "$$\\begin{align}\n",
    "\\inf_{q_{\\R{y}|\\RM{x}}} H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}})\n",
    "&= H(\\R{y}|\\RM{x})\n",
    "\\end{align}$$\n",
    "\n",
    "and the optimal solution equals $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02870c39",
   "metadata": {},
   "source": [
    "Hence, a neural network that minimizes the cross entropy equals $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$ and any possible input image $\\RM{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d6746",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "::::{exercise}\n",
    ":label: ex:train:1\n",
    "Prove the above proposition using the information identity and the property of divergence that \n",
    "\n",
    "$$\\begin{align}\n",
    "D(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}})\\geq 0 \\tag{positivity of divergence}\n",
    "\\end{align}$$\n",
    "\n",
    "with equality iff $q_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s.\n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "\n",
    "The positivity of divergence can be proved using the [log-sum inequality](https://en.wikipedia.org/wiki/Log_sum_inequality)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{i} a_i \\log\\left(\\frac{a_i}{b_i}\\right) \\geq (\\textstyle \\sum_{i} a_i) \\log\\left(\\frac{\\sum_{i} a_i}{\\sum_{i} b_i}\\right)\\tag{log-sum inequality}\n",
    "\\end{align}$$ \n",
    "\n",
    "for any sequences $\\{a_i\\}$, $\\{b_i\\}$, and $\\{c_i\\}$.\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97d38d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3237f6317457954f58b14d1e2702de5a",
     "grade": true,
     "grade_id": "consistency",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baff71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The cross entropy cannot be computed exactly without knowing the joint distribution $p_{\\RM{x}\\R{y}}$. Nevertheless, it can be estimated from a batch of i.i.d. samples $(\\RM{x}_{\\R{j}_i},\\R{y}_{\\R{j}_i})$ for $1\\leq i\\leq n$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\R{L}(\\theta)&:=\\frac1n \\sum_{i=1}^n \\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}_{\\R{j}_i}|\\RM{x}_{\\R{j}_i})}\\tag{empirical loss}\n",
    "\\end{align}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\theta := \\operatorname{flat}(\\M{W}^{(\\ell)},\\M{b}^{(\\ell)}\\mid 0\\leq \\ell \\leq L)$$\n",
    "\n",
    "is the vector of parameters of the neural network defined in (net)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414d78b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A mini-batch [gradient descent algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is often used to reduce the loss. It iteratively updates/trains the neural network parameters:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta -s\\nabla \\R{L}(\\theta)$$\n",
    "\n",
    "by computing the gradient $\\nabla \\R{L}(\\theta)$ on a randomly selected minibatch of examples and choosing an appropriate learning rate $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57825e4",
   "metadata": {},
   "source": [
    "What is gradient descent?\n",
    "\n",
    "::::{card}\n",
    ":header: [open in new tab](https://www.youtube.com/embed/IHZwWFHWa-w?start=468&end=510)\n",
    ":::{iframe} https://www.youtube.com/embed/IHZwWFHWa-w?start=468&end=510\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab500af",
   "metadata": {},
   "source": [
    "How to choose the step size?\n",
    "\n",
    "::::{card}\n",
    ":header: [open in new tab](https://www.youtube.com/embed/IHZwWFHWa-w?start=403&end=415)\n",
    ":::{iframe} https://www.youtube.com/embed/IHZwWFHWa-w?start=403&end=415\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f879be",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The gradient can be computed systematically using a technique called *backpropagation* due to the structure of the neural network in (net).\n",
    "- The learning rate can affect the convergence rate of the loss to a local minima: \n",
    "    - $\\theta$ may overshoot its optimal value if $s$ is too large, and\n",
    "    - the convergence can be very slow if $\\theta$ is too small.\n",
    "\n",
    "A more advanced method called [Adam (Adaptive Momentum Estimation)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) can adaptively choose $s$ to speed up the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75525f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45669851",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The [loss function](https://keras.io/api/losses/), gradient descent algorithm, and the performance metrics can be specified using the [`compile` method](https://keras.io/api/losses/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf4fad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "compile_model(model)\n",
    "model.loss, model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a98b32c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can train the neural network using the method [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) of the compiled model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f559fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"NBGRADER_EXECUTION\"):\n",
    "    model.fit(ds_b[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec8c6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "::::{exercise}\n",
    ":label: ex:train:2\n",
    "\n",
    "By default, the neural network is trained for 1 epoch. What happens to the training accuracy if you rerun the above cell to train the model for another epoch?\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163a843",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3359d5d7ef71d2a3d7de7c29c5b7f0a1",
     "grade": true,
     "grade_id": "retrain",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d7da4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can set the parameter `epochs` to train the neural network for multiple epochs since it is quite unlikely to train a neural network well with just one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f017e56",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "To determine whether the neural network is well-trained (when to stop training), we should also use a separate validation set to evaluate the performance of the neural network. The validation set can be specified using the parameter `validation_set` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e78f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"NBGRADER_EXECUTION\"):\n",
    "    model.fit(ds_b[\"train\"], epochs=6, validation_data=ds_b[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bcb5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "::::{exercise} \n",
    ":label: ex:train:3\n",
    "Is the maximum validation accuracy `val_sparse_categorical_accuracy` (over different epoches) an unbiased estimate of the performance of deep learning for the given dataset? If not, how to avoid the bias?\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07017309",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "336aded134c9923168791d9525d47565",
     "grade": true,
     "grade_id": "bias",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ea909",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cca14",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once you are satisfied with the result, you can deploy the model as a web application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28acab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `mnist` folder contain the webpage `index.html` that\n",
    "- presents an HTML5 canvas for users to input a handwritten digit,\n",
    "- loads a trained model using [`tensorflow.js`](https://www.tensorflow.org/js),\n",
    "- passes the handwritten digit to the model to predict the distribution of the digit types, and\n",
    "- display the most likely digit type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca964d97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "::::{literalinclude} mnist/index.html\n",
    ":language: html\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d8447",
   "metadata": {
    "editable": true,
    "hideCode": true,
    "hidePrompt": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "display.Code(\"mnist/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685e121",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then, convert the model to files that can be loaded by [`tensorflow.js`](https://www.tensorflow.org/js). First, save the model as follows in [HDF5](https://www.h5py.org/) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96370932-c376-400e-a619-0e182187d54d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"NBGRADER_EXECUTION\"):\n",
    "    !mkdir -p private\n",
    "    model.save(\"private/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f97990",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Then convert it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e184ed-ce2d-4392-9553-81b053d34fe4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"NBGRADER_EXECUTION\"):\n",
    "    !tensorflowjs_converter --input_format keras 'private/model.h5' 'mnist/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f35873",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "To host the web application, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca3444",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"NBGRADER_EXECUTION\"):\n",
    "    !mkdir -p ~/www/ && cp -r mnist ~/www/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17312",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "View your web app here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf31cc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "display.IFrame(src=JUPYTER_SERVICE_PREFIX + \"www/mnist/\", width=500, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db29de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "::::{caution}\n",
    "Remember to release the resources if it is no longer used. You can release the memory or GPU memory by `Kernel->Shut Down Kernel`.\n",
    "::::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "skip_execution": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
