{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**DIVE into Deep Learning**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:03.503341Z",
     "start_time": "2021-05-21T10:23:03.490144Z"
    },
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why can we learn from examples?**$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the problem to be meaningful, $(\\RM{x},\\R{y})$ is assumed to be random with some unknown joint distribution $p_{\\RM{x},\\R{y}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we always had $\\R{y}=y$ instead, then a perfect classifier can just return $y$ without even looking at $\\RM{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $p_{\\RM{x},\\R{y}}$ were known instead, then $p_{\\R{y}|\\RM{x}}$ would also be known and therefore needed not be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples are often collected randomly and independently from a population, i.e., as [i.i.d. samples](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) of $(\\RM{x},\\R{y})$:\n",
    "\n",
    "$$(\\RM{x}_1,\\R{y}_1), (\\RM{x}_2,\\R{y}_2), \\dots\\sim_{\\text{i.i.d.}} p_{\\RM{x},\\R{y}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If all the examples were the same instead, the examples could not show the pattern of how $\\R{y}$ depends on $\\RM{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The observed distribution of i.i.d. samples converge to the unknown distribution by the [law of large number](https://en.wikipedia.org/wiki/Law_of_large_numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How to determine if a classifier is good?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ultimately, we desire a classifier with the maximum accuracy in predicting $\\R{y}$ but doing so is [computationally too difficult](https://en.wikipedia.org/wiki/Loss_functions_for_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, we regard a classification algorithm to be reasonably good if \n",
    "- it can achieve the maximum possible accuracy \n",
    "- as the number of training samples goes to $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition** A probabilistic classifier for the input feature $\\RM{x}$ and label $\\R{y}$ with unknown joint distribution  is a conditional pmf\n",
    "\n",
    "$$\n",
    "\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})\\qquad \\text{for }\\M{x}\\in \\mc{X}, y\\in \\mc{Y}\n",
    "$$\n",
    "\n",
    "defined as a function of the i.i.d. samples\n",
    "\n",
    "$$\\{(\\RM{x}_i,\\R{y}_i)\\}_{i=1}^N$$\n",
    "\n",
    "of $(\\RM{x},\\R{y})$ (but independent of $(\\RM{x},\\R{y})$). \n",
    "The classifier is said to be a *consistent* estimate (of $p_{\\R{y}|\\M{x}}$) if\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{N\\to \\infty} \\Pr\\Set{\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x})\\text{ for all } y\\in \\mc{Y}}=1,\\tag{consistency}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "namely, $\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})$ converges almost surely (a.s.) to $p_{\\R{y}|\\RM{x}}$. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A consistent probabilistic classifier gives rise to an asymptotically optimal hard-decision classifier that achieves the maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Proposition** If for some $\\epsilon\\geq 0$ that\n",
    "\n",
    "$$\\Pr\\Set{\\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x}) \\text{ for all } y\\in \\mc{Y}}=1-\\epsilon,$$\n",
    "\n",
    "the (hard-decision) classifier\n",
    "\n",
    "$$\\begin{align}\\R{f}(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} \\R{q}_{\\R{y}|\\RM{x}}(y|\\RM{x})\\tag{hard decision}\\end{align}$$\n",
    "\n",
    "achieves an accuracy\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sup_{f:\\mc{X}\\to \\mc{Y}} \\Pr(\\R{y}= f(\\RM{x})) &\\geq \\E\\left[\\max_{y\\in \\mc{Y}} p_{\\R{y}|\\M{x}}(y|\\RM{x})\\right] - \\epsilon,\\tag{accuracy lower bound}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the expectation is the maximum possible accuracy. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Proof:* For any classifier $f$,\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\Pr(\\R{y}= f(\\RM{x}))\n",
    "&= \\E[p_{\\R{y}|\\M{x}}(f(\\RM{x})|\\RM{x})] \\\\\n",
    "&\\leq \\E\\left[\\max_{y\\in \\mc{Y}} p_{\\R{y}|\\M{x}}(y|\\RM{x})\\right]\\end{align*}\n",
    "$$\n",
    "\n",
    "where the last inequality is achievable with equality with the hard-decision classifier and $\\R{q}$ replaced by $p$. This implies the desired accuracy lower bound for the case $\\epsilon=0$. The more general case with $\\epsilon\\geq 0$ can be derived similarly. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How can we obtain a consistent classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We train a neural network to minimize certain *loss* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common loss function for classification uses the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) measure in information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The theoretical underpinning is the following identity that relates three information quantities:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\overbrace{\\E\\left[\\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{ \\text{Cross entropy}\\\\ H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=} &\\equiv \\overbrace{\\E\\left[\\log \\frac{1}{p_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right]}^{\\text{Conditional entropy}\\\\ H(\\R{y}|\\RM{x}):=} + \\overbrace{\\E\\left[\\log \\frac{p_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}{q_{\\R{y}|\\RM{x}}(\\R{y}|\\RM{x})}\\right].}^{\\text{Divergence}\\\\ D(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}}):=}\\tag{identity}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The identity can be proved quite easily using the linearity of expectation \n",
    "\n",
    "$$ \\E[\\R{u}+\\R{v}]=\\E[\\R{u}]+\\E[\\R{v}],$$\n",
    "\n",
    "and a property of logarithm that \n",
    "\n",
    "$$\\log uv = \\log u+ \\log v.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:03.512769Z",
     "start_time": "2021-05-21T10:23:03.506604Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hidePrompt": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<b>Information Identity</b><br>\n",
    "<video width=\"100%\" height=\"450\" controls>\n",
    "      <source src=\"https://www.cs.cityu.edu.hk/~ccha23/dl/InformationIdentity.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Proposition** With $q_{\\R{y}|\\RM{x}}(y|\\M{x})$ being a valid pmf of a random variable taking values from $\\mc{Y}$ conditioned on a random variable taking values from $\\mc{X}$, \n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{q_{\\R{y}|\\RM{x}}} H(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}})\n",
    "&= H(\\R{y}|\\RM{x})\n",
    "\\end{align}$$\n",
    "\n",
    "and the optimal solution equals $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$. $\\square$ \n",
    "\n",
    "Hence, a neural network that minimizes the cross entropy equals $p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. for all $y\\in \\mc{Y}$ and any possible input image $\\RM{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise** Prove the above proposition using the information identity and the property of divergence that \n",
    "\n",
    "$$\\begin{align}\n",
    "D(p_{\\R{y}|\\RM{x}}\\|q_{\\R{y}|\\RM{x}}|p_{\\RM{x}})\\geq 0 \\tag{positivity of divergence}\n",
    "\\end{align}$$\n",
    "\n",
    "with equality iff $q_{\\R{y}|\\RM{x}}(y|\\RM{x})=p_{\\R{y}|\\RM{x}}(y|\\RM{x})$ a.s. (This, in turn, can be proved using the [log-sum inequality](https://en.wikipedia.org/wiki/Log_sum_inequality):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{i} a_i \\log\\left(\\frac{a_i}{b_i}\\right) \\geq (\\textstyle \\sum_{i} a_i) \\log\\left(\\frac{\\sum_{i} a_i}{\\sum_{i} b_i}\\right)\\tag{log-sum inequality}\n",
    "\\end{align}$$ \n",
    "\n",
    "for any sequences $\\{a_i\\}$, $\\{b_i\\}$, and $\\{c_i\\}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3237f6317457954f58b14d1e2702de5a",
     "grade": true,
     "grade_id": "consistency",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The cross entropy cannot be computed exactly without knowing the joint distribution $p_{\\RM{x}\\R{y}}$. Nevertheless, it can be estimated from a batch of i.i.d. samples $(\\RM{x}_{\\R{j}_i},\\R{y}_{\\R{j}_i})$ for $1\\leq i\\leq n$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\R{L}(\\theta)&:=\\frac1n \\sum_{i=1}^n \\log \\frac{1}{q_{\\R{y}|\\RM{x}}(\\R{y}_{\\R{j}_i}|\\RM{x}_{\\R{j}_i})}\\tag{empirical loss}\n",
    "\\end{align}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\theta := \\operatorname{flat}(\\M{W}^{(\\ell)},\\M{b}^{(\\ell)}\\mid 0\\leq \\ell \\leq L)$$\n",
    "\n",
    "is the vector of parameters of the neural network defined in (net)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A mini-batch [gradient descent algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is often used to reduce the loss. It iteratively updates/trains the neural network parameters:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta -s\\nabla \\R{L}(\\theta)$$\n",
    "\n",
    "by computing the gradient $\\nabla \\R{L}(\\theta)$ on a randomly selected minibatch of examples and choosing an appropriate learning rate $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:03.519789Z",
     "start_time": "2021-05-21T10:23:03.515170Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hidePrompt": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div>\n",
    "<b>What is gradient descent?</b><br>\n",
    "<iframe width=\"100%\" height=\"450\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=468&end=510\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "<div>\n",
    "<b>How to choose the step size?</b><br>\n",
    "<iframe width=\"100%\" height=\"450\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=403&end=415\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The gradient can be computed systematically using a technique called *backpropagation* due to the structure of the neural network in (net).\n",
    "- The learning rate can affect the convergence rate of the loss to a local minima: \n",
    "    - $\\theta$ may overshoot its optimal value if $s$ is too large, and\n",
    "    - the convergence can be very slow if $\\theta$ is too small.\n",
    "\n",
    "A more advanced method called [Adam (Adaptive Momentum Estimation)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) can adaptively choose $s$ to speed up the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The [loss function](https://keras.io/api/losses/), gradient descent algorithm, and the performance metrics can be specified using the [`compile` method](https://keras.io/api/losses/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:03.540042Z",
     "start_time": "2021-05-21T10:23:03.521940Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "compile_model(model)\n",
    "model.loss, model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can train the neural network using the method [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) of the compiled model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:25.633361Z",
     "start_time": "2021-05-21T10:23:03.542035Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    model.fit(ds_b[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise** By default, the neural network is trained for 1 epoch. What happens to the training accuracy if you rerun the above cell to train the model for another epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3359d5d7ef71d2a3d7de7c29c5b7f0a1",
     "grade": true,
     "grade_id": "retrain",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can set the parameter `epochs` to train the neural network for multiple epochs since it is quite unlikely to train a neural network well with just one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To determine whether the neural network is well-trained (when to stop training), we should also use a separate validation set to evaluate the performance of the neural network. The validation set can be specified using the parameter `validation_set` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:42.086828Z",
     "start_time": "2021-05-21T10:23:25.636869Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if input('Train? [Y/n]').lower() != 'n':\n",
    "    model.fit(ds_b[\"train\"], epochs=6, validation_data=ds_b[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise** Is the maximum validation accuracy `val_sparse_categorical_accuracy` (over different epoches) an unbiased estimate of the performance of deep learning for the given dataset? If not, how to avoid the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "336aded134c9923168791d9525d47565",
     "grade": true,
     "grade_id": "bias",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T11:05:07.830407Z",
     "start_time": "2021-02-26T11:05:07.827690Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once you are satisfied with the result, you can deploy the model as a web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `mnist` folder contain the webpage `index.html` that\n",
    "- presents an HTML5 canvas for users to input a handwritten digit,\n",
    "- loads a trained model using [`tensorflow.js`](https://www.tensorflow.org/js),\n",
    "- passes the handwritten digit to the model to predict the distribution of the digit types, and\n",
    "- display the most likely digit type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:42.124342Z",
     "start_time": "2021-05-21T10:23:42.090701Z"
    },
    "hideCode": true,
    "hidePrompt": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "display.Code('mnist/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, save the model as follows in [HDF5](https://www.h5py.org/) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:43.321313Z",
     "start_time": "2021-05-21T10:23:42.126985Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    model.save('mnist/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, convert the model to files that can be loaded by [`tensorflow.js`](https://www.tensorflow.org/js):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:56.420896Z",
     "start_time": "2021-05-21T10:23:43.324149Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    !tensorflowjs_converter --input_format keras 'mnist/model.h5' 'mnist/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To host the web application, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:23:58.321449Z",
     "start_time": "2021-05-21T10:23:56.429765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if input('Execute? [Y/n]').lower() != 'n':\n",
    "    !cp -rf mnist ~/www/\n",
    "    !zip -r mnist.zip mnist/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Preview you web application at [\\<JUPYTERHUB_URL\\>/user-redirect/www/mnist/](/user-redirect/www/mnist/).\n",
    "- Download [`mnist.zip`](./mnist.zip) and extract it to a static web server of your choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
