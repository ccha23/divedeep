{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**DIVE into Deep Learning**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:37.488700Z",
     "start_time": "2021-05-21T10:24:37.483645Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:37.495950Z",
     "start_time": "2021-05-21T10:24:37.490617Z"
    },
    "hideCode": true,
    "hidePrompt": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.cs.cityu.edu.hk/~ccha23/playground\" width=\"100%\" height=\"800\" frameBorder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As shown below, a neural network is organized into layers of computation units called the *neurons*.$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For $\\ell\\in \\{0,\\dots,L\\}$ and integer $L\\geq 1$, let \n",
    "- $\\M{a}^{(\\ell)}$ be the output of the $\\ell$-th layer of the neural network, and\n",
    "- $a^{(\\ell)}_i$ be the $i$-th element of $\\M{a}^{(\\ell)}$. The element is computed from the output $\\M{a}^{(\\ell-1)}$ of its previous layer except for $\\ell=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $0$-th layer is called the *input layer*, i.e.,\n",
    "$$\\M{a}^{(0)}:=\\M{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $L$-th layer $\\M{a}^{(L)}$ is called the *output layer*. All other layers are called the *hidden layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What should be the neural network output?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal is to train a *classifier* that predicts a label $\\R{y}$ for an input feature $\\RM{x}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of a hard-decision classifier is a function $f:\\mc{X}\\to \\mc{Y}$ such that\n",
    "$f(\\RM{x})$ predicts $\\R{y}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we train a probabilistic classifier $q_{\\R{y}|\\RM{x}}$ that estimates $p_{\\R{y}|\\RM{x}}$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "[q_{\\R{y}|\\RM{x}}(y|\\M{x})]_{y\\in \\mc{Y}} &:= \\M{a}^{(L)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For MNIST, the goal is to classify the digit type of a handwritten digit.  When given a handwritten digit,\n",
    "- a hard-decision classifier returns a digit type, and\n",
    "- a probabilistic classifier returns a distribution of the digit types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why train a probabilistic classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A probabilistic classifer is more general and can give a hard decision as well   \n",
    "\n",
    "  $$f(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} q_{\\R{y}|\\RM{x}}(y|\\RM{x})$$ \n",
    "  by returning the estimated most likely digit type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A neural network can model the distribution $p_{\\R{y}|\\RM{x}}(\\cdot|\\RM{x})$ better than $\\R{y}$ because its output is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How to ensure $\\M{a}^{(L)}$ is a valid probability vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The soft-max activation function is often used for the last layer:\n",
    "\n",
    "$$ \\sigma^{(L)}\\left(\\left[\\begin{smallmatrix}z^{(\\ell)}_1 \\\\ \\vdots \\\\ z^{(\\ell)}_k\\end{smallmatrix}\\right]\\right) := \\frac{1}{\\sum_{i=1}^k e^{z^{(\\ell)}_i}} \\left[\\begin{smallmatrix}e^{z^{(\\ell)}_1} \\\\ \\vdots \\\\ e^{z^{(\\ell)}_k}\\end{smallmatrix}\\right] $$ \n",
    "\n",
    "where $k:=\\abs{\\mc{Y}}=10$ is the number of distinct class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It follows that: \n",
    "\n",
    "$$\\sum_{i=1}^k a_i^{(L)}  = 1\\kern1em \\text{and} \\kern1em a_i^{(L)}\\geq 0\\qquad \\forall i\\in \\{1,\\dots,k\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How are the different layers related?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\M{a}^{(\\ell)}&:=\\begin{cases}\n",
    "\\M{x} & \\ell=0\\\\\n",
    "\\sigma^{(\\ell)}(\\overbrace{\\M{W}^{(\\ell)}\\M{a}^{(\\ell-1)}+\\M{b}^{(\\ell)}}^{\\RM{z}^{(\\ell)}:=})& \\ell>0;\n",
    "\\end{cases}\\tag{net}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\M{W}^{(\\ell)}$ is a matrix of weights;\n",
    "- $\\M{b}^{(\\ell)}$ is a vector called bias; and\n",
    "- $\\sigma^{(\\ell)}$ is a reveal-valued function called the *activation function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The activation functions $\\sigma^{(\\ell)}$ for other layers $1\\leq \\ell<L$ is often the vectorized version of \n",
    "-  sigmoid:  \n",
    "\n",
    "  $$\\sigma_{\\text{sigmoid}}(z) = \\frac{1}{1+e^{-z}}$$\n",
    "-  rectified linear unit (ReLU): \n",
    "\n",
    "  $$ \\sigma_{\\text{ReLU}}(z) = \\max\\{0,z\\}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:37.502879Z",
     "start_time": "2021-05-21T10:24:37.497886Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hidePrompt": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<b>Activation function:</b><br>\n",
    "<iframe width=\"100%\" height=\"450\" src=\"https://www.youtube.com/embed/aircAruvnKk?start=649&end=695\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following plots the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:37.887051Z",
     "start_time": "2021-05-21T10:24:37.505792Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.max([np.zeros(z.shape), z], axis=0)\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.plot(z, ReLU(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r'ReLU: $\\max\\{0,z\\}$')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T10:40:54.078963Z",
     "start_time": "2021-02-22T10:40:54.074180Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise** Complete the vectorized function `sigmoid` using the vectorized exponentiation `np.exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:38.057709Z",
     "start_time": "2021-05-21T10:24:37.889198Z"
    },
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f299b139d55fccf5a920653eb24edf1",
     "grade": false,
     "grade_id": "sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "plt.plot(z, ReLU(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r'Sigmoid function: $\\frac{1}{1+e^{-z}}$')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:38.064989Z",
     "start_time": "2021-05-21T10:24:38.059885Z"
    },
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa581795d83f22d1d7242e5de68f5ab",
     "grade": true,
     "grade_id": "test-sigmoid",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following uses the [`keras`](https://keras.io/) library to define the basic neural network achitecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`keras` runs on top of `tensorflow` and offers a higher-level abstraction to simplify the construction and training of a neural network. ([`tflearn`](https://github.com/tflearn/tflearn) is another library that provides a higher-level API for `tensorflow`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:38.106698Z",
     "start_time": "2021-05-21T10:24:38.066945Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    tf.keras.backend.clear_session() # clear keras cache. \n",
    "                        # See https://github.com/keras-team/keras/issues/7294\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
    "    ], 'Simple_sequential')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_simple_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The above defines [a linear stack](https://www.tensorflow.org/api_docs/python/tf/keras/layers) of [fully-connected layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) after [flattening the input](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten). The method `summary` is useful for [debugging in Keras](https://keras.io/examples/keras_recipes/debugging_tips/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Execise** Assign to `n_hidden_layers` the number of hidden layers for the above simple sequential model. \n",
    "\n",
    "*Hint:* The layer `Flatten` does not count as a hidden layer since it simply reshapes the input without using any trainable parameters. The output layer also does not count as a hidden layer since its output is the output of the neural network, not intermediate (hidden) values that require further processing by the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:38.116301Z",
     "start_time": "2021-05-21T10:24:38.109882Z"
    },
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ed972ecc58c3a66402e21da68bc57e",
     "grade": false,
     "grade_id": "simple-seq",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "n_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T10:24:38.121950Z",
     "start_time": "2021-05-21T10:24:38.118135Z"
    },
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a506eca5aeefcc6be556f35873038c4",
     "grade": true,
     "grade_id": "test-simple-seq",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
