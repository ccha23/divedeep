{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0762d1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "title: Network Architecture\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79fb28",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**DIVE into Deep Learning**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844354c-dfb2-4cff-892f-89e53a122436",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a67510",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde87e82",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30729983",
   "metadata": {},
   "source": [
    "::::{card}\n",
    ":header: [open in new tab](https://www.cs.cityu.edu.hk/~ccha23/playground)\n",
    ":::{iframe} https://www.cs.cityu.edu.hk/~ccha23/playground\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a7bc1",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As shown below, a neural network is organized into layers of computation units called the *neurons*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9e68c",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For $\\ell\\in \\{0,\\dots,L\\}$ and integer $L\\geq 1$, let \n",
    "- $\\M{a}^{(\\ell)}$ be the output of the $\\ell$-th layer of the neural network, and\n",
    "- $a^{(\\ell)}_i$ be the $i$-th element of $\\M{a}^{(\\ell)}$. The element is computed from the output $\\M{a}^{(\\ell-1)}$ of its previous layer except for $\\ell=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b651d",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $0$-th layer is called the *input layer*, i.e.,\n",
    "$$\\M{a}^{(0)}:=\\M{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a8dfa",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $L$-th layer $\\M{a}^{(L)}$ is called the *output layer*. All other layers are called the *hidden layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99473cee",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What should be the neural network output?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee92785",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal is to train a *classifier* that predicts a label $\\R{y}$ for an input feature $\\RM{x}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747d24c",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of a hard-decision classifier is a function $f:\\mc{X}\\to \\mc{Y}$ such that\n",
    "$f(\\RM{x})$ predicts $\\R{y}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea702db",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we train a probabilistic classifier $q_{\\R{y}|\\RM{x}}$ that estimates $p_{\\R{y}|\\RM{x}}$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "[q_{\\R{y}|\\RM{x}}(y|\\M{x})]_{y\\in \\mc{Y}} &:= \\M{a}^{(L)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b7d5d",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the MNIST dataset, a common goal is to classify the digit type of a handwritten digit.  When given a handwritten digit,\n",
    "- a hard-decision classifier returns a digit type, and\n",
    "- a probabilistic classifier returns a distribution of the digit types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316963e",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why train a probabilistic classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2d623",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A probabilistic classifer is more general and it can give a hard decision as well   \n",
    "\n",
    "  $$f(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} q_{\\R{y}|\\RM{x}}(y|\\RM{x})$$ \n",
    "  by returning the estimated most likely digit type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57665f7b",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A neural network can model the distribution $p_{\\R{y}|\\RM{x}}(\\cdot|\\RM{x})$ better than $\\R{y}$ because its output is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73ba91",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How to ensure $\\M{a}^{(L)}$ is a valid probability vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20da1d8",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The soft-max activation function is often used for the last layer:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\sigma^{(L)}\\left(\\left[\\begin{smallmatrix}z^{(\\ell)}_1 \\\\ \\vdots \\\\ z^{(\\ell)}_k\\end{smallmatrix}\\right]\\right) := \\frac{1}{\\sum_{i=1}^k e^{z^{(\\ell)}_i}} \\left[\\begin{smallmatrix}e^{z^{(\\ell)}_1} \\\\ \\vdots \\\\ e^{z^{(\\ell)}_k}\\end{smallmatrix}\\right]\\tag{soft-max} \n",
    "\\end{align}$$ \n",
    "\n",
    "where $k:=\\abs{\\mc{Y}}=10$ is the number of distinct class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0cc88c",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It follows that: \n",
    "\n",
    "$$\\sum_{i=1}^k a_i^{(L)}  = 1\\kern1em \\text{and} \\kern1em a_i^{(L)}\\geq 0\\qquad \\forall i\\in \\{1,\\dots,k\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f47676",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How are the different layers related?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044ecbc",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\M{a}^{(\\ell)}&:=\\begin{cases}\n",
    "\\M{x} & \\ell=0\\\\\n",
    "\\sigma^{(\\ell)}(\\overbrace{\\M{W}^{(\\ell)}\\M{a}^{(\\ell-1)}+\\M{b}^{(\\ell)}}^{\\RM{z}^{(\\ell)}:=})& \\ell>0;\n",
    "\\end{cases}\\tag{net}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\M{W}^{(\\ell)}$ is a matrix of weights;\n",
    "- $\\M{b}^{(\\ell)}$ is a vector called bias; and\n",
    "- $\\sigma^{(\\ell)}$ is a reveal-valued function called the *activation function*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df346c14",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The activation functions $\\sigma^{(\\ell)}$ for other layers $1\\leq \\ell<L$ is often the vectorized version of \n",
    "-  sigmoid:  \n",
    "\n",
    "  $$\\sigma_{\\text{sigmoid}}(z) = \\frac{1}{1+e^{-z}}$$\n",
    "-  rectified linear unit (ReLU): \n",
    "\n",
    "  $$ \\sigma_{\\text{ReLU}}(z) = \\max\\{0,z\\}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994df42",
   "metadata": {},
   "source": [
    "::::{card}\n",
    ":header: [open in new tab](https://www.youtube.com/embed/aircAruvnKk?start=649&end=695)\n",
    ":::{iframe} https://www.youtube.com/embed/aircAruvnKk?start=649&end=695\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a14ad4",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following plots the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a128939",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.max([np.zeros(z.shape), z], axis=0)\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.figure(num=1)\n",
    "plt.plot(z, ReLU(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r\"ReLU: $\\max\\{0,z\\}$\")\n",
    "plt.xlabel(r\"$z$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3f724",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{exercise}\n",
    ":label: ex:1\n",
    "Complete the vectorized function `sigmoid` using the vectorized exponentiation `np.exp`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd438621",
   "metadata": {
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb73bbd60abb02fcf3d517fcc8dd6aca",
     "grade": false,
     "grade_id": "sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.figure(num=2)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r\"Sigmoid function: $\\frac{1}{1+e^{-z}}$\")\n",
    "plt.xlabel(r\"$z$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163c8ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa581795d83f22d1d7242e5de68f5ab",
     "grade": true,
     "grade_id": "test-sigmoid",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26bc7df",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078b33b",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following uses the [`keras`](https://keras.io/) library to define the basic neural network achitecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20e8fd",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`keras` runs on top of `tensorflow` and offers a higher-level abstraction to simplify the construction and training of a neural network. ([`tflearn`](https://github.com/tflearn/tflearn) is another library that provides a higher-level API for `tensorflow`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc88ad7",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    tf.keras.backend.clear_session()  # clear keras cache.\n",
    "    # See https://github.com/keras-team/keras/issues/7294\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax),\n",
    "        ],\n",
    "        \"Simple_sequential\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_simple_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c0458",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The above defines [a linear stack](https://www.tensorflow.org/api_docs/python/tf/keras/layers) of [fully-connected layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) after [flattening the input](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten). The method `summary` is useful for [debugging in Keras](https://keras.io/examples/keras_recipes/debugging_tips/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38356a5f",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "::::{exercise}\n",
    ":label: ex:2\n",
    "Assign to `n_hidden_layers` the number of hidden layers for the above simple sequential model. \n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "The layer `Flatten` does not count as a hidden layer since it simply reshapes the input without using any trainable parameters. The output layer also does not count as a hidden layer since its output is the output of the neural network, not intermediate (hidden) values that require further processing by the neurons.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72cedb",
   "metadata": {
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8609d2cf29c556fe519a4de5ab22a65a",
     "grade": false,
     "grade_id": "simple-seq",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "n_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff60c20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a506eca5aeefcc6be556f35873038c4",
     "grade": true,
     "grade_id": "test-simple-seq",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86036bb9",
   "metadata": {},
   "source": [
    "::::{important}\n",
    "Remember to release the resources if it is no longer used. You can release the memory or GPU memory by `Kernel->Shut Down Kernel`.\n",
    "::::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
