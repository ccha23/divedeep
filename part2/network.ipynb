{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c271ca5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "title: Network Architecture\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0820e6b",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**DIVE into Deep Learning**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b28c73",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6c4fc",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce771a",
   "metadata": {},
   "source": [
    "::::{card}\n",
    ":header: [open in new tab](https://www.cs.cityu.edu.hk/~ccha23/playground)\n",
    ":::{iframe} https://www.cs.cityu.edu.hk/~ccha23/playground\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d788db",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As shown below, a neural network is organized into layers of computation units called the *neurons*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a501d8",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For $\\ell\\in \\{0,\\dots,L\\}$ and integer $L\\geq 1$, let \n",
    "- $\\M{a}^{(\\ell)}$ be the output of the $\\ell$-th layer of the neural network, and\n",
    "- $a^{(\\ell)}_i$ be the $i$-th element of $\\M{a}^{(\\ell)}$. The element is computed from the output $\\M{a}^{(\\ell-1)}$ of its previous layer except for $\\ell=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2402d4a",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $0$-th layer is called the *input layer*, i.e.,\n",
    "$$\\M{a}^{(0)}:=\\M{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc617526",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $L$-th layer $\\M{a}^{(L)}$ is called the *output layer*. All other layers are called the *hidden layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04dd71",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What should be the neural network output?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b119d4c",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal is to train a *classifier* that predicts a label $\\R{y}$ for an input feature $\\RM{x}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdb5cf",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of a hard-decision classifier is a function $f:\\mc{X}\\to \\mc{Y}$ such that\n",
    "$f(\\RM{x})$ predicts $\\R{y}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b3f4d",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we train a probabilistic classifier $q_{\\R{y}|\\RM{x}}$ that estimates $p_{\\R{y}|\\RM{x}}$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "[q_{\\R{y}|\\RM{x}}(y|\\M{x})]_{y\\in \\mc{Y}} &:= \\M{a}^{(L)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebb90e",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the MNIST dataset, a common goal is to classify the digit type of a handwritten digit.  When given a handwritten digit,\n",
    "- a hard-decision classifier returns a digit type, and\n",
    "- a probabilistic classifier returns a distribution of the digit types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd0748",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why train a probabilistic classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bea89",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A probabilistic classifer is more general and it can give a hard decision as well   \n",
    "\n",
    "  $$f(\\RM{x}):=\\arg\\max_{y\\in \\mc{Y}} q_{\\R{y}|\\RM{x}}(y|\\RM{x})$$ \n",
    "  by returning the estimated most likely digit type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7cda9",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A neural network can model the distribution $p_{\\R{y}|\\RM{x}}(\\cdot|\\RM{x})$ better than $\\R{y}$ because its output is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20f641",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How to ensure $\\M{a}^{(L)}$ is a valid probability vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c0194",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The soft-max activation function is often used for the last layer:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\sigma^{(L)}\\left(\\left[\\begin{smallmatrix}z^{(\\ell)}_1 \\\\ \\vdots \\\\ z^{(\\ell)}_k\\end{smallmatrix}\\right]\\right) := \\frac{1}{\\sum_{i=1}^k e^{z^{(\\ell)}_i}} \\left[\\begin{smallmatrix}e^{z^{(\\ell)}_1} \\\\ \\vdots \\\\ e^{z^{(\\ell)}_k}\\end{smallmatrix}\\right]\\tag{soft-max} \n",
    "\\end{align}$$ \n",
    "\n",
    "where $k:=\\abs{\\mc{Y}}=10$ is the number of distinct class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1823f5a",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It follows that: \n",
    "\n",
    "$$\\sum_{i=1}^k a_i^{(L)}  = 1\\kern1em \\text{and} \\kern1em a_i^{(L)}\\geq 0\\qquad \\forall i\\in \\{1,\\dots,k\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4f3dc",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How are the different layers related?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06deecd",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\M{a}^{(\\ell)}&:=\\begin{cases}\n",
    "\\M{x} & \\ell=0\\\\\n",
    "\\sigma^{(\\ell)}(\\overbrace{\\M{W}^{(\\ell)}\\M{a}^{(\\ell-1)}+\\M{b}^{(\\ell)}}^{\\RM{z}^{(\\ell)}:=})& \\ell>0;\n",
    "\\end{cases}\\tag{net}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\M{W}^{(\\ell)}$ is a matrix of weights;\n",
    "- $\\M{b}^{(\\ell)}$ is a vector called bias; and\n",
    "- $\\sigma^{(\\ell)}$ is a reveal-valued function called the *activation function*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da013d9",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The activation functions $\\sigma^{(\\ell)}$ for other layers $1\\leq \\ell<L$ is often the vectorized version of \n",
    "-  sigmoid:  \n",
    "\n",
    "  $$\\sigma_{\\text{sigmoid}}(z) = \\frac{1}{1+e^{-z}}$$\n",
    "-  rectified linear unit (ReLU): \n",
    "\n",
    "  $$ \\sigma_{\\text{ReLU}}(z) = \\max\\{0,z\\}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a37805",
   "metadata": {},
   "source": [
    "::::{card}\n",
    ":header: [open in new tab](https://www.youtube.com/embed/aircAruvnKk?start=649&end=695)\n",
    ":::{iframe} https://www.youtube.com/embed/aircAruvnKk?start=649&end=695\n",
    ":width: 100%\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937ff4d",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following plots the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f031a0f",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.max([np.zeros(z.shape), z], axis=0)\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.figure(num=1)\n",
    "plt.plot(z, ReLU(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r\"ReLU: $\\max\\{0,z\\}$\")\n",
    "plt.xlabel(r\"$z$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb5db4",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{exercise}\n",
    ":label: ex:1\n",
    "Complete the vectorized function `sigmoid` using the vectorized exponentiation `np.exp`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d910a0",
   "metadata": {
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88f78d2436e46a5936ca873965a1e8ed",
     "grade": false,
     "grade_id": "sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "plt.figure(num=2)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.xlim(-5, 5)\n",
    "plt.title(r\"Sigmoid function: $\\frac{1}{1+e^{-z}}$\")\n",
    "plt.xlabel(r\"$z$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fb18c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa581795d83f22d1d7242e5de68f5ab",
     "grade": true,
     "grade_id": "test-sigmoid",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227438b7",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c4752",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following uses the [`keras`](https://keras.io/) library to define the basic neural network achitecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341b958",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`keras` runs on top of `tensorflow` and offers a higher-level abstraction to simplify the construction and training of a neural network. ([`tflearn`](https://github.com/tflearn/tflearn) is another library that provides a higher-level API for `tensorflow`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8365a4",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    tf.keras.backend.clear_session()  # clear keras cache.\n",
    "    # See https://github.com/keras-team/keras/issues/7294\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax),\n",
    "        ],\n",
    "        \"Simple_sequential\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_simple_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7be2d",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The above defines [a linear stack](https://www.tensorflow.org/api_docs/python/tf/keras/layers) of [fully-connected layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) after [flattening the input](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten). The method `summary` is useful for [debugging in Keras](https://keras.io/examples/keras_recipes/debugging_tips/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fa063",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "::::{exercise}\n",
    ":label: ex:2\n",
    "Assign to `n_hidden_layers` the number of hidden layers for the above simple sequential model. \n",
    "\n",
    ":::{hint}\n",
    ":class: dropdown\n",
    "The layer `Flatten` does not count as a hidden layer since it simply reshapes the input without using any trainable parameters. The output layer also does not count as a hidden layer since its output is the output of the neural network, not intermediate (hidden) values that require further processing by the neurons.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16038c03",
   "metadata": {
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ed972ecc58c3a66402e21da68bc57e",
     "grade": false,
     "grade_id": "simple-seq",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "n_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02f504",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a506eca5aeefcc6be556f35873038c4",
     "grade": true,
     "grade_id": "test-simple-seq",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e405c",
   "metadata": {},
   "source": [
    "::::{important}\n",
    "Remember to release the resources if it is no longer used. You can release the memory or GPU memory by `Kernel->Shut Down Kernel`.\n",
    "::::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
